{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib64/python2.7/site-packages/cffi/model.py:532: UserWarning: 'point_conversion_form_t' has no values explicitly defined; guessing that it is equivalent to 'unsigned int'\n",
      "  % self._get_c_name())\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 256\n",
    "def read_image(f):\n",
    "    im = Image.open(f)\n",
    "    im = im.resize((size, size), PIL.Image.NEAREST)\n",
    "    im = np.asarray(im, dtype='float64')\n",
    "    return(im)\n",
    "    \n",
    "train_files = []\n",
    "val_files = []\n",
    "test_files = []\n",
    "\n",
    "train_files = os.listdir('/train_images')\n",
    "val_files = os.listdir('/val_images')\n",
    "test_files = os.listdir('/test_images')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Geometric_0.jpg', 'Typography_1.jpg', 'Solid_2.jpg', 'Striped_3.jpg', 'Abstract_4.jpg']\n",
      "['Typography_0.jpg', 'Striped_1.jpg', 'People and Places_2.jpg', 'Solid_3.jpg', 'Solid_4.jpg']\n",
      "['Solid_0.jpg', 'Solid_1.jpg', 'Solid_2.jpg', 'Solid_3.jpg', 'Solid_4.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(train_files[:5])\n",
    "print(val_files[:5])\n",
    "print(test_files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Solid_2126.jpg'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_files[2125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_files = len(train_files)\n",
    "print('Total number of train images:', n_train_files)\n",
    "\n",
    "n_val_files = len(val_files)\n",
    "print('Total number of val images:', n_val_files)\n",
    "\n",
    "i=0\n",
    "x_train = []\n",
    "x_val = []\n",
    "print('Reading train images ...')\n",
    "for file in train_files:\n",
    "    im = read_image('/train_images/'+file)\n",
    "    x_train.append(im)\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "print('Reading val images ...')\n",
    "for file in val_files:\n",
    "    im = read_image('/val_images/'+file)\n",
    "    x_val.append(im)\n",
    "    i+=1\n",
    "\n",
    "print(\"Done reading all images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total number of test images:', 2849)\n",
      "Reading test images ...\n",
      "Done reading all images\n"
     ]
    }
   ],
   "source": [
    "n_test_files = len(test_files)\n",
    "print('Total number of test images:', n_test_files)\n",
    "\n",
    "i=0\n",
    "x_test = []\n",
    "print('Reading test images ...')\n",
    "for file in test_files:\n",
    "    im = read_image('/test_images/'+file)\n",
    "    x_test.append(im)\n",
    "    i+=1\n",
    "\n",
    "print(\"Done reading all images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((2849, 256, 256, 3), 'x_test Shape')\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(x_train)\n",
    "print(x_train.shape, 'x_train Shape')\n",
    "\n",
    "x_val = np.array(x_val)\n",
    "print(x_val.shape, 'x_val Shape')\n",
    "\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "print(x_test.shape, 'x_test Shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"train_preprocessed.npy\",x_train)\n",
    "np.save(\"val_preprocessed.npy\",x_val)\n",
    "#can load using np.load\n",
    "# a = np.load('xyz.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255.\n",
    "x_val /= 255.\n",
    "x_test /= 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"train_normalized.npy\",x_train)\n",
    "np.save(\"val_normalized.npy\",x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"test_normalized.npy\",x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('train_normalized.npy')\n",
    "x_val = np.load('val_normalized.npy')\n",
    "x_test = np.load('test_normalized.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "y_val = []\n",
    "y_test = []\n",
    "\n",
    "for file in val_files:\n",
    "    y_val.append(file.split('_')[0])\n",
    "\n",
    "for file in test_files:\n",
    "    y_test.append(file.split('_')[0])\n",
    "        \n",
    "for file in train_files:\n",
    "    y_train.append(file.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder  \n",
    "\n",
    "le = LabelEncoder()\n",
    "y_val = le.fit_transform(y_val)\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Checked': 3, 'Camouflage': 2, 'Sports': 15, 'Music': 10, 'Polka Dots': 12, 'Superhero': 17, 'Geometric': 7, 'People and Places': 11, 'Colourblocked': 4, 'Self Design': 13, 'Humour and Comic': 9, 'Varsity': 21, 'Floral': 6, 'Striped': 16, 'Tie and Dye': 18, 'Conversational': 5, 'Graphic': 8, 'Solid': 14, 'Tribal': 19, 'Typography': 20, 'Abstract': 0, 'Biker': 1}\n"
     ]
    }
   ],
   "source": [
    "keys = le.classes_\n",
    "values = le.transform(le.classes_)\n",
    "dictionary = dict(zip(keys, values))\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((5649, 22), 'y_train Shape')\n",
      "((997, 22), 'y_val Shape')\n",
      "((2849, 22), 'y_test Shape')\n"
     ]
    }
   ],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "print(y_train.shape, 'y_train Shape')\n",
    "\n",
    "y_val = to_categorical(y_val)\n",
    "print(y_val.shape, 'y_val Shape')\n",
    "\n",
    "y_test = to_categorical(y_test)\n",
    "print(y_test.shape, 'y_test Shape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model using inception arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get inception architecture from keras.applications\n",
    "from keras.applications.inception_v3 import InceptionV3, decode_predictions\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "trained_model = InceptionV3(include_top=False,weights='imagenet')\n",
    "# print(trained_model.summary())\n",
    "\n",
    "x = trained_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(300,activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "pred_inception= Dense(22,activation='softmax')(x)\n",
    "model = Model(inputs=trained_model.input,outputs=pred_inception)\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the layers of inception non-trainable\n",
    "for layer in trained_model.layers:\n",
    "    layer.trainable=False\n",
    "    \n",
    "#compiling the model\n",
    "adam = Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5649 samples, validate on 997 samples\n",
      "Epoch 1/10\n",
      "5649/5649 [==============================] - 26s 5ms/step - loss: 1.7314 - acc: 0.5849 - val_loss: 2.9649 - val_acc: 0.3681\n",
      "Epoch 2/10\n",
      "5649/5649 [==============================] - 19s 3ms/step - loss: 1.0503 - acc: 0.7123 - val_loss: 2.4173 - val_acc: 0.4243\n",
      "Epoch 3/10\n",
      "5649/5649 [==============================] - 19s 3ms/step - loss: 0.8293 - acc: 0.7483 - val_loss: 2.4472 - val_acc: 0.4273\n",
      "Epoch 4/10\n",
      "5649/5649 [==============================] - 20s 3ms/step - loss: 0.7108 - acc: 0.7816 - val_loss: 2.5750 - val_acc: 0.4514\n",
      "Epoch 5/10\n",
      "5649/5649 [==============================] - 19s 3ms/step - loss: 0.6107 - acc: 0.8005 - val_loss: 2.4784 - val_acc: 0.5336\n",
      "Epoch 6/10\n",
      "5649/5649 [==============================] - 20s 4ms/step - loss: 0.5622 - acc: 0.8143 - val_loss: 3.1639 - val_acc: 0.4293\n",
      "Epoch 7/10\n",
      "5649/5649 [==============================] - 20s 3ms/step - loss: 0.5285 - acc: 0.8223 - val_loss: 2.5500 - val_acc: 0.5737\n",
      "Epoch 8/10\n",
      "5649/5649 [==============================] - 20s 3ms/step - loss: 0.4774 - acc: 0.8400 - val_loss: 3.8599 - val_acc: 0.3761\n",
      "Epoch 9/10\n",
      "5649/5649 [==============================] - 20s 4ms/step - loss: 0.4430 - acc: 0.8499 - val_loss: 3.4553 - val_acc: 0.4363\n",
      "Epoch 10/10\n",
      "5649/5649 [==============================] - 21s 4ms/step - loss: 0.4148 - acc: 0.8584 - val_loss: 3.0194 - val_acc: 0.4303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x86fec50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size= 64, epochs= 10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def model_train(optimizer, lr_rate, freez, nb_epoch=2):\n",
    "    tag = 'model_'+optimizer+'_lr_'+str(lr_rate)+'_freeze_'+str(freez)\n",
    "    trained_model = InceptionV3(include_top=False,weights='imagenet')\n",
    "    x = trained_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    pred_inception= Dense(22,activation='softmax')(x)\n",
    "    model = Model(inputs=trained_model.input,outputs=pred_inception)\n",
    "\n",
    "    #making the layers non-trainable\n",
    "    for layer in trained_model.layers:\n",
    "        layer.trainable=freez\n",
    "\n",
    "    #compiling the model\n",
    "    if optimizer=='adam':\n",
    "        optim = Adam(lr=lr_rate)\n",
    "    elif optimizer=='sgd':\n",
    "        optim = SGD(lr=lr_rate)\n",
    "    model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer=optim)\n",
    "\n",
    "    train_datagen = ImageDataGenerator(rescale=None, \n",
    "                                       height_shift_range=0.2, \n",
    "                                       width_shift_range=0.2,\n",
    "                                       horizontal_flip=True, \n",
    "                                       rotation_range=10)\n",
    "    validation_datagen = ImageDataGenerator(rescale=None)\n",
    "\n",
    "    train_generator = train_datagen.flow(x=x_train, y=y_train,batch_size=batch_size)\n",
    "    validation_generator = validation_datagen.flow(x=x_val, y=y_val,batch_size=batch_size)\n",
    "\n",
    "    hist = model.fit_generator(train_generator,\n",
    "                        steps_per_epoch=train_generator.n//batch_size, # '//' in python returns only the quotient\n",
    "                        epochs=nb_epoch,\n",
    "                        validation_data=validation_generator,\n",
    "                        validation_steps=validation_generator.n//batch_size, \n",
    "                        verbose=0).history\n",
    "    \n",
    "    model.save(tag+'.h5')\n",
    "    np.save(tag+'.npy', hist)\n",
    "    \n",
    "    t_acc = np.min(hist['acc'])\n",
    "    v_acc = np.min(hist['val_acc'])\n",
    "    return(t_acc, v_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_list = ['adam', 'sgd']\n",
    "lr_rate_list = [0.01,0.001]\n",
    "freez_list = [True, False]\n",
    "\n",
    "gs_list = list(itertools.product(optimizer_list, lr_rate_list, freez_list))\n",
    "print(gs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_acc = []\n",
    "val_acc = []\n",
    "for op, lr, fr in gs_list:\n",
    "    print('Running combination - Optimizer:{}, Learning Rate:{}, Freeze Weights:{}'.format(op, lr, fr))\n",
    "    r1, r2 = model_train(op, lr, fr)\n",
    "    tr_acc.append(r1)\n",
    "    val_acc.append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(gs_list)\n",
    "df.columns = ['Optimizer', 'Learning Rate', 'Freeze Weights']\n",
    "df['Train Acc'] = tr_acc\n",
    "df['Val Acc'] = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r1, r2 = model_train('sgd', 0.010, True)\n",
    "tr_acc.append(r1)\n",
    "val_acc.append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception tensorflow.python.framework.errors_impl.InvalidArgumentError: InvalidArgumentError() in <bound method _Callable.__del__ of <tensorflow.python.client.session._Callable object at 0xd77a5e10>> ignored\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor of shape [] and type float\n\t [[Node: batch_normalization_1361_1/moving_mean/local_step/Initializer/zeros = Const[_class=[\"loc:@batch_normalization_1361_1/moving_mean/Assign\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 0>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op u'batch_normalization_1361_1/moving_mean/local_step/Initializer/zeros', defined at:\n  File \"/usr/lib64/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib64/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib64/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/lib64/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/lib64/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib64/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib64/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib64/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib64/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-42-445f91adb291>\", line 2, in <module>\n    model = load_model('model_sgd_lr_0.01_freeze_True.h5')\n  File \"/usr/lib64/python2.7/site-packages/keras/engine/saving.py\", line 260, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"/usr/lib64/python2.7/site-packages/keras/engine/saving.py\", line 334, in model_from_config\n    return deserialize(config, custom_objects=custom_objects)\n  File \"/usr/lib64/python2.7/site-packages/keras/layers/__init__.py\", line 55, in deserialize\n    printable_module_name='layer')\n  File \"/usr/lib64/python2.7/site-packages/keras/utils/generic_utils.py\", line 145, in deserialize_keras_object\n    list(custom_objects.items())))\n  File \"/usr/lib64/python2.7/site-packages/keras/engine/network.py\", line 1027, in from_config\n    process_node(layer, node_data)\n  File \"/usr/lib64/python2.7/site-packages/keras/engine/network.py\", line 986, in process_node\n    layer(unpack_singleton(input_tensors), **kwargs)\n  File \"/usr/lib64/python2.7/site-packages/keras/engine/base_layer.py\", line 457, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/usr/lib64/python2.7/site-packages/keras/layers/normalization.py\", line 195, in call\n    self.momentum),\n  File \"/usr/lib64/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1012, in moving_average_update\n    x, value, momentum, zero_debias=True)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/moving_averages.py\", line 89, in assign_moving_average\n    update_delta = _zero_debias(variable, value, decay)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/moving_averages.py\", line 218, in _zero_debias\n    trainable=False)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1467, in get_variable\n    aggregation=aggregation)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1217, in get_variable\n    aggregation=aggregation)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 527, in get_variable\n    aggregation=aggregation)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 481, in _true_getter\n    aggregation=aggregation)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 903, in _get_single_variable\n    aggregation=aggregation)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2443, in variable\n    aggregation=aggregation)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2425, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2406, in default_variable_creator\n    constraint=constraint)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 259, in __init__\n    constraint=constraint)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 368, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 885, in <lambda>\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/init_ops.py\", line 100, in __call__\n    return array_ops.zeros(shape, dtype)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1539, in zeros\n    output = _constant_if_small(zero, shape, dtype, name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1497, in _constant_if_small\n    return constant(value, shape=shape, dtype=dtype, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 202, in constant\n    name=name).outputs[0]\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [] and type float\n\t [[Node: batch_normalization_1361_1/moving_mean/local_step/Initializer/zeros = Const[_class=[\"loc:@batch_normalization_1361_1/moving_mean/Assign\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 0>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-445f91adb291>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_sgd_lr_0.01_freeze_True.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/keras/engine/saving.pyc\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/keras/engine/saving.pyc\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m    926\u001b[0m                              ' elements.')\n\u001b[1;32m    927\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2438\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2440\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [] and type float\n\t [[Node: batch_normalization_1361_1/moving_mean/local_step/Initializer/zeros = Const[_class=[\"loc:@batch_normalization_1361_1/moving_mean/Assign\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 0>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op u'batch_normalization_1361_1/moving_mean/local_step/Initializer/zeros', defined at:\n  File \"/usr/lib64/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib64/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib64/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/lib64/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/lib64/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib64/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib64/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib64/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib64/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-42-445f91adb291>\", line 2, in <module>\n    model = load_model('model_sgd_lr_0.01_freeze_True.h5')\n  File \"/usr/lib64/python2.7/site-packages/keras/engine/saving.py\", line 260, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"/usr/lib64/python2.7/site-packages/keras/engine/saving.py\", line 334, in model_from_config\n    return deserialize(config, custom_objects=custom_objects)\n  File \"/usr/lib64/python2.7/site-packages/keras/layers/__init__.py\", line 55, in deserialize\n    printable_module_name='layer')\n  File \"/usr/lib64/python2.7/site-packages/keras/utils/generic_utils.py\", line 145, in deserialize_keras_object\n    list(custom_objects.items())))\n  File \"/usr/lib64/python2.7/site-packages/keras/engine/network.py\", line 1027, in from_config\n    process_node(layer, node_data)\n  File \"/usr/lib64/python2.7/site-packages/keras/engine/network.py\", line 986, in process_node\n    layer(unpack_singleton(input_tensors), **kwargs)\n  File \"/usr/lib64/python2.7/site-packages/keras/engine/base_layer.py\", line 457, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/usr/lib64/python2.7/site-packages/keras/layers/normalization.py\", line 195, in call\n    self.momentum),\n  File \"/usr/lib64/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1012, in moving_average_update\n    x, value, momentum, zero_debias=True)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/moving_averages.py\", line 89, in assign_moving_average\n    update_delta = _zero_debias(variable, value, decay)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/moving_averages.py\", line 218, in _zero_debias\n    trainable=False)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1467, in get_variable\n    aggregation=aggregation)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1217, in get_variable\n    aggregation=aggregation)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 527, in get_variable\n    aggregation=aggregation)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 481, in _true_getter\n    aggregation=aggregation)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 903, in _get_single_variable\n    aggregation=aggregation)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2443, in variable\n    aggregation=aggregation)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2425, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2406, in default_variable_creator\n    constraint=constraint)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 259, in __init__\n    constraint=constraint)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 368, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 885, in <lambda>\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/init_ops.py\", line 100, in __call__\n    return array_ops.zeros(shape, dtype)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1539, in zeros\n    output = _constant_if_small(zero, shape, dtype, name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1497, in _constant_if_small\n    return constant(value, shape=shape, dtype=dtype, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 202, in constant\n    name=name).outputs[0]\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [] and type float\n\t [[Node: batch_normalization_1361_1/moving_mean/local_step/Initializer/zeros = Const[_class=[\"loc:@batch_normalization_1361_1/moving_mean/Assign\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 0>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('model_sgd_lr_0.01_freeze_True.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tried genetic algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named gentun",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-659e415e4170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgentun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgentun\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneticCnnIndividual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRussianRouletteGA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named gentun"
     ]
    }
   ],
   "source": [
    "    import random\n",
    "    import gentun\n",
    "\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    from gentun import Population, GeneticCnnIndividual, RussianRouletteGA\n",
    "\n",
    "    n = x_train.shape[0]\n",
    "    selection = random.sample(range(n), 10000)  # Use only a subsample\n",
    "    x_train = x_train.reshape(n, 28, 28, 1)[selection]\n",
    "\n",
    "    pop = Population(\n",
    "        GeneticCnnIndividual, x_train, y_train, size=20, crossover_rate=0.3, mutation_rate=0.1,\n",
    "        additional_parameters={\n",
    "            'kfold': 5, 'epochs': (20, 4, 1), 'learning_rate': (1e-3, 1e-4, 1e-5), 'batch_size': 32\n",
    "        }, maximize=True\n",
    "    )\n",
    "    ga = RussianRouletteGA(pop, crossover_probability=0.2, mutation_probability=0.8)\n",
    "    ga.run(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(filters=32, kernel_size=(3, 3), padding='same', \n",
    "                        input_shape=(256,256,3), \n",
    "                        activation='relu'))\n",
    "model.add(Convolution2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Convolution2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(22, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5649 samples, validate on 997 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception tensorflow.python.framework.errors_impl.InvalidArgumentError: InvalidArgumentError() in <bound method _Callable.__del__ of <tensorflow.python.client.session._Callable object at 0xd77b0650>> ignored\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor of shape [] and type float\n\t [[Node: loss_1/mul/x = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 1>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-0d51bfe62a49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2633\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2635\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2636\u001b[0m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2585\u001b[0m         \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2586\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2587\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2588\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2589\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1412\u001b[0m     \"\"\"\n\u001b[1;32m   1413\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m           self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[0;32m-> 1368\u001b[0;31m               session._session, options_ptr, status)\n\u001b[0m\u001b[1;32m   1369\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [] and type float\n\t [[Node: loss_1/mul/x = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 1>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=10,\n",
    "          validation_data=(x_val, y_val),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
